{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de05d3a2",
   "metadata": {},
   "source": [
    "# Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ae783",
   "metadata": {},
   "source": [
    "It is an algorithm to perform optimization and is the most common way to optimize neural network . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ec7b9",
   "metadata": {},
   "source": [
    "It minimize the Loss function parameterized by model's parameters by updating the parameters in the opposite direction of the gradient of the loss function with respect to the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002aba9",
   "metadata": {},
   "source": [
    "# Types of gradient descents \n",
    "\n",
    "1. Batch Gradient Descent \n",
    "2. Stochastic gradient descent \n",
    "3. Mini batch gradient descent \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca800f",
   "metadata": {},
   "source": [
    "<h3>Batch gradient descent  :</h3> \n",
    "It processess entire row of dataset once and then update the weight and bias in the same number as of the number of epochs \n",
    "\n",
    "Batch gradient descne t : model.fit(X_train, y_train, epochs=10, batch_size=n )\n",
    "\n",
    "n = No of rows in the dataset \n",
    "\n",
    "let's say i have 100 rows of dataset then : \n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=100)\n",
    "\n",
    "\n",
    "<h3> Stochastic gradient descent : </h3>\n",
    "It process one row each time and then update the weight and bias row by row shuffeling the dataset . \n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=1)\n",
    "\n",
    "Batch size is set to 1 for stochastic gradient Descent  . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77184743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d4ba056",
   "metadata": {},
   "source": [
    "# Vanishing Gradient Problem \n",
    "\n",
    "\n",
    "It is the problem encountered when training the artificial networks with gradent based learning methods and backprogragation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b5892",
   "metadata": {},
   "source": [
    "# early stopping in deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355029d",
   "metadata": {},
   "source": [
    "It is a regularization technique in deep learning hta prevents overfitting by monitoring a model's performance on a seperate validation dataset and halting training when validation performance starts to degrade , even if training error keeps failingg "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f9049",
   "metadata": {},
   "source": [
    "<h3> WHat it does ? </h3>\n",
    "\n",
    "1. It splits the data into training and validation sets : \n",
    "\n",
    "2. During the training the model's loss or accuracy is tracked on the validation set after each epoch\n",
    "\n",
    "3. When the validation loss stops decreasing for a specified number of epochs called patience then Training is paused. \n",
    "\n",
    "4. The model's weight fro the epoch with the best validation performance are saved and used . \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87465b",
   "metadata": {},
   "source": [
    "<h3> Why is it important </h3>\n",
    "\n",
    " 1. It prevents overfitting  by stopping the model from learning the training data's noise . \n",
    "\n",
    " 2. Saves time and resources by avoiding training for long time in large models. \n",
    "\n",
    " 3. The model performs better on general unseen real world data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f26a0",
   "metadata": {},
   "source": [
    "Patience = no of epochs to wait after having continuous accuracy values . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d71ed",
   "metadata": {},
   "source": [
    "Min delta = minimum amount of change required to quality as a real improvement in model performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b78089",
   "metadata": {},
   "source": [
    "# Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35353be",
   "metadata": {},
   "source": [
    "It is a technique that introduce randomness to the neural network during training. \n",
    "\n",
    "It randomly drops some neurons that doesnot contribut in forward propagation during training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139e22f",
   "metadata": {},
   "source": [
    "It is done to control overfitting \n",
    "\n",
    "To account for the deactivated neurons, the outputs of the remaining active neurons are scaled up by a factor equal to the probability of keeping a neuron active (e.g., if 50% are dropped, the remaining ones are multiplied by 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b86e65",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
